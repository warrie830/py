#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç½‘é¡µè‡ªåŠ¨ä¿å­˜å·¥å…·
æ”¯æŒä¿å­˜ä»»æ„ç½‘é¡µå’Œè±†ç“£å¸–å­çš„HTMLæ–‡æ¡£
"""

import os
import re
import time
import requests
from urllib.parse import urlparse, urljoin
from datetime import datetime
from pathlib import Path
import json
from typing import Optional, Dict, Any
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('web_saver.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class WebSaver:
    """ç½‘é¡µä¿å­˜å™¨"""
    
    def __init__(self, output_dir: str = "saved_pages"):
        """
        åˆå§‹åŒ–ç½‘é¡µä¿å­˜å™¨
        
        Args:
            output_dir: ä¿å­˜ç›®å½•
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # è±†ç“£ç‰¹å®šçš„è¯·æ±‚å¤´
        self.douban_headers = self.headers.copy()
        self.douban_headers.update({
            'Referer': 'https://www.douban.com/',
            'Cookie': 'bid=' + ''.join([chr(random.randint(65, 90)) for _ in range(11)])
        })
        
        # ä¼šè¯å¯¹è±¡ï¼Œä¿æŒè¿æ¥
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def get_page_content(self, url: str, is_douban: bool = False) -> Optional[str]:
        """
        è·å–ç½‘é¡µå†…å®¹
        
        Args:
            url: ç½‘é¡µURL
            is_douban: æ˜¯å¦ä¸ºè±†ç“£é¡µé¢
            
        Returns:
            ç½‘é¡µHTMLå†…å®¹
        """
        try:
            headers = self.douban_headers if is_douban else self.headers
            response = self.session.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            # è®¾ç½®æ­£ç¡®çš„ç¼–ç 
            response.encoding = response.apparent_encoding or 'utf-8'
            
            logger.info(f"æˆåŠŸè·å–é¡µé¢: {url}")
            return response.text
            
        except requests.RequestException as e:
            logger.error(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None
    
    def is_douban_url(self, url: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºè±†ç“£URL
        
        Args:
            url: ç½‘é¡µURL
            
        Returns:
            æ˜¯å¦ä¸ºè±†ç“£é¡µé¢
        """
        domain = urlparse(url).netloc.lower()
        return 'douban.com' in domain
    
    def generate_filename(self, url: str, title: str = None) -> str:
        """
        ç”Ÿæˆæ–‡ä»¶å
        
        Args:
            url: ç½‘é¡µURL
            title: é¡µé¢æ ‡é¢˜
            
        Returns:
            æ–‡ä»¶å
        """
        # ä»URLä¸­æå–åŸŸåå’Œè·¯å¾„
        parsed = urlparse(url)
        domain = parsed.netloc.replace('.', '_')
        path = parsed.path.strip('/').replace('/', '_')
        
        # å¦‚æœæ²¡æœ‰è·¯å¾„ï¼Œä½¿ç”¨åŸŸå
        if not path:
            path = domain
        
        # æ·»åŠ æ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # å¦‚æœæœ‰æ ‡é¢˜ï¼Œä½¿ç”¨æ ‡é¢˜ï¼ˆæ¸…ç†ç‰¹æ®Šå­—ç¬¦ï¼‰
        if title:
            # æ¸…ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦
            title = re.sub(r'[<>:"/\\|?*]', '_', title)
            title = title[:50]  # é™åˆ¶é•¿åº¦
            filename = f"{title}_{timestamp}.html"
        else:
            filename = f"{domain}_{path}_{timestamp}.html"
        
        return filename
    
    def extract_title(self, html_content: str) -> str:
        """
        ä»HTMLä¸­æå–æ ‡é¢˜
        
        Args:
            html_content: HTMLå†…å®¹
            
        Returns:
            é¡µé¢æ ‡é¢˜
        """
        # å°è¯•ä»titleæ ‡ç­¾æå–
        title_match = re.search(r'<title[^>]*>(.*?)</title>', html_content, re.IGNORECASE | re.DOTALL)
        if title_match:
            title = title_match.group(1).strip()
            if title:
                return title
        
        # å°è¯•ä»h1æ ‡ç­¾æå–
        h1_match = re.search(r'<h1[^>]*>(.*?)</h1>', html_content, re.IGNORECASE | re.DOTALL)
        if h1_match:
            title = h1_match.group(1).strip()
            if title:
                return title
        
        return ""
    
    def save_page(self, url: str, custom_title: str = None) -> bool:
        """
        ä¿å­˜ç½‘é¡µ
        
        Args:
            url: ç½‘é¡µURL
            custom_title: è‡ªå®šä¹‰æ ‡é¢˜
            
        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            # åˆ¤æ–­æ˜¯å¦ä¸ºè±†ç“£é¡µé¢
            is_douban = self.is_douban_url(url)
            
            # è·å–é¡µé¢å†…å®¹
            html_content = self.get_page_content(url, is_douban)
            if not html_content:
                return False
            
            # æå–æ ‡é¢˜
            title = custom_title or self.extract_title(html_content)
            
            # ç”Ÿæˆæ–‡ä»¶å
            filename = self.generate_filename(url, title)
            filepath = self.output_dir / filename
            
            # ä¿å­˜HTMLæ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                'url': url,
                'title': title,
                'saved_at': datetime.now().isoformat(),
                'is_douban': is_douban,
                'file_size': len(html_content)
            }
            
            metadata_file = filepath.with_suffix('.json')
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            logger.info(f"é¡µé¢å·²ä¿å­˜: {filepath}")
            logger.info(f"å…ƒæ•°æ®å·²ä¿å­˜: {metadata_file}")
            
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜é¡µé¢å¤±è´¥ {url}: {e}")
            return False
    
    def save_douban_post(self, url: str) -> bool:
        """
        ä¸“é—¨ä¿å­˜è±†ç“£å¸–å­
        
        Args:
            url: è±†ç“£å¸–å­URL
            
        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        logger.info(f"æ­£åœ¨ä¿å­˜è±†ç“£å¸–å­: {url}")
        return self.save_page(url, custom_title="è±†ç“£å¸–å­")
    
    def batch_save(self, urls: list, delay: float = 1.0) -> Dict[str, bool]:
        """
        æ‰¹é‡ä¿å­˜ç½‘é¡µ
        
        Args:
            urls: URLåˆ—è¡¨
            delay: è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
            
        Returns:
            ä¿å­˜ç»“æœå­—å…¸
        """
        results = {}
        
        for i, url in enumerate(urls, 1):
            logger.info(f"æ­£åœ¨å¤„ç†ç¬¬ {i}/{len(urls)} ä¸ªURL: {url}")
            
            success = self.save_page(url)
            results[url] = success
            
            # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            if i < len(urls):
                time.sleep(delay)
        
        return results


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 50)
    print("ç½‘é¡µè‡ªåŠ¨ä¿å­˜å·¥å…·")
    print("=" * 50)
    
    # åˆ›å»ºä¿å­˜å™¨
    saver = WebSaver()
    
    while True:
        print("\nè¯·é€‰æ‹©æ“ä½œ:")
        print("1. ä¿å­˜å•ä¸ªç½‘é¡µ")
        print("2. ä¿å­˜è±†ç“£å¸–å­")
        print("3. æ‰¹é‡ä¿å­˜ç½‘é¡µ")
        print("4. æŸ¥çœ‹å·²ä¿å­˜çš„æ–‡ä»¶")
        print("5. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-5): ").strip()
        
        if choice == '1':
            url = input("è¯·è¾“å…¥ç½‘é¡µURL: ").strip()
            if url:
                success = saver.save_page(url)
                if success:
                    print("âœ… é¡µé¢ä¿å­˜æˆåŠŸ!")
                else:
                    print("âŒ é¡µé¢ä¿å­˜å¤±è´¥!")
        
        elif choice == '2':
            url = input("è¯·è¾“å…¥è±†ç“£å¸–å­URL: ").strip()
            if url:
                success = saver.save_douban_post(url)
                if success:
                    print("âœ… è±†ç“£å¸–å­ä¿å­˜æˆåŠŸ!")
                else:
                    print("âŒ è±†ç“£å¸–å­ä¿å­˜å¤±è´¥!")
        
        elif choice == '3':
            print("è¯·è¾“å…¥URLåˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰:")
            urls = []
            while True:
                url = input().strip()
                if not url:
                    break
                urls.append(url)
            
            if urls:
                delay = float(input("è¯·è¾“å…¥è¯·æ±‚é—´éš”ï¼ˆç§’ï¼Œé»˜è®¤1.0ï¼‰: ") or "1.0")
                results = saver.batch_save(urls, delay)
                
                print("\næ‰¹é‡ä¿å­˜ç»“æœ:")
                for url, success in results.items():
                    status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
                    print(f"{url}: {status}")
        
        elif choice == '4':
            print(f"\nå·²ä¿å­˜çš„æ–‡ä»¶ï¼ˆä¿å­˜åœ¨ {saver.output_dir}ï¼‰:")
            for file in saver.output_dir.glob("*.html"):
                size = file.stat().st_size
                print(f"ğŸ“„ {file.name} ({size} bytes)")
        
        elif choice == '5':
            print("æ„Ÿè°¢ä½¿ç”¨ï¼")
            break
        
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥ï¼")


if __name__ == "__main__":
    import random
    main() 